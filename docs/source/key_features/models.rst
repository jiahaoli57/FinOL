.. _supported_models:

Pre-built Models
================

.. seealso::
   See :mod:`~finol.model_layer` for more details.

As a software package, part of the innovation of ``FinOL`` is the pre-implemented models in its model layer.
These pre-built models serves as a significant advantage for researchers in the data-driven OLPS domain,
offering them a solid foundation to build upon.
By leveraging these models, researchers can streamline the initial phases of their projects,
circumventing the need to build complex models from scratch and thereby conserving valuable time and resources.


Time Series Representation Models
---------------------------------

The time series representation models within ``FinOL`` are implemented to address the unique challenges of
sequential data in finance. These models are specifically designed to handle sequential inputs,
ensuring that the temporal dependencies inherent in financial time series data are accurately captured and analyzed.
By leveraging these models, researchers can gain deeper insights into market trends, price movements,
and other critical factors that influence data-driven OLPS task.


AlphaPortfolio
~~~~~~~~~~~~~~

.. figure:: ../../images/models/AlphaPortfolio.png
   :align: center
   :width: 550px

   Overall Framework of AlphaPortfolio


The AlphaPortfolio model is a Transformer-based model for asset scoring and portfolio selection. It consists of two
main components:

1. Sequence Representations Extraction Module (SREM): This module takes the input features for each asset over a
time window and generates a fixed-size embedding vector to represent the asset.

2. Cross Asset Attention Network (CAAN): This module takes the sequence representations generated by the SREM and
applies cross-asset attention to produce the final asset scores.

The AlphaPortfolio model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset. The final output of the AlphaPortfolio model is a tensor of shape ``(batch_size, num_assets)``,
represents the predicted score for the corresponding asset.

For more details, please refer to the paper `AlphaPortfolio: Direct Construction through Reinforcement Learning
and Interpretable AI <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3698800>`__.

.. table:: Table C.1: Hyperparameters of TE-CAAN-Based AP.
    :class: ghost

    +----------------------+--------+-----------------+--------+
    | Hyper-parameter      | Choice | Hyper-parameter | Choice |
    +======================+========+=================+========+
    | Embedding dimension  | 256    | Optimizer       | SGD    |
    +----------------------+--------+-----------------+--------+
    | Feed-forward network | 1021   | Learning rate   | 0.0001 |
    +----------------------+--------+-----------------+--------+
    | Number of multi-head | 4      | Dropout ratio   | 0.2    |
    +----------------------+--------+-----------------+--------+
    | Number of TE layer   | 1      | Training epochs | 30     |
    +----------------------+--------+-----------------+--------+


AlphaStock
~~~~~~~~~~~~~~~~~~~~

.. figure:: ../../images/models/AlphaStock.png
   :align: center
   :width: 350px

   Key Component of AlphaStock

The AlphaStock model is a LSTM-based model for asset scoring and portfolio selection. It consists of two
main components:

1. Long Short-Term Memory with History state Attention (LSTM-HA): This module takes the input features for each asset
over a time window and generates a fixed-size vector to represent the asset.
A major advantage of LSTM-HA is that it can learn both the sequential and global dependences from stock history states.
Compared with the existing studies that only use a recurrent neural network to extract the sequential dependence in history states

2. Cross Asset Attention Network (CAAN): This module takes the sequence representations generated by the LSTM-HA and
applies cross-asset attention to produce the final asset scores.

The AlphaStock model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset. The final output of the AlphaStock model is a tensor of shape ``(batch_size, num_assets)``,
where each element represents the predicted score for the corresponding asset.

For more details, please refer to the paper `AlphaStock: A Buying-Winners-and-Selling-Losers Investment
Strategy using Interpretable Deep Reinforcement Attention Networks <https://dl.acm.org/doi/abs/10.1145/3292500.3330647>`__.

DNN
~~~~~~~~~~~~~~~~~~~~

The DNN model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset. The model applies a series of fully connected layers to the input,
with each layer followed by a ReLU activation and a dropout layer.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced DNN-based models.

GRU
~~~

The GRU model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced GRU-based models.


LSRE-CAAN
~~~~~~~~~

.. figure:: ../../images/models/LSRE-CAAN.jpg
   :align: center
   :width: 550px

   Overall Framework of LSRE-CAAN

The LSRE-CAAN model is a Transformer-based model for asset scoring and portfolio selection. It consists of two
main components:

1. Long Sequence Representations Extractor (LSRE): This module uses a Transformer-based architecture to extract asset
temporal representation. In addition, LSRE introduces a small set of latent units on top of the original Transformer
Encoder to form an attention bottleneck through which the input must pass, which not only effectively solves the
original Transformer Encoder's quadratic complexity problem.

2. Cross Asset Attention Network (CAAN): This module takes the sequence representations generated by the LSRE and
applies cross-asset attention to produce the final asset scores.

The LSRE-CAAN model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset. The final output of the LSRE-CAAN model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

For more details, please refer to the paper `Online portfolio management via deep reinforcement learning with high-frequency data
<https://www.sciencedirect.com/science/article/abs/pii/S030645732200348X>`__.

.. table:: Table 7: Hyper-parameters of the LSRE-CAAN framework.
    :class: ghost

    +---------------------------+---------------+------------------------------------------------------------------+
    | Hyper-parameter           | Choice        | Description                                                      |
    +===========================+===============+==================================================================+
    | Depth of net (L)          | 1             | The number of process layers in LSRE.                            |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Number of latents (M)     | 1             | The number of latents.                                           |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Latent dimension (D)      | 32            | The size of the latent space.                                    |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Number of cross-heads     | 1             | The number of heads for cross-attention.                         |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Number of latent-heads    | 1             | The number of heads for latent self-attention.                   |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Cross-attention dimension | 64            | The number of dimensions per cross-attention head.               |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Self-attention dimension  | 32            | The number of dimensions per latent self-attention head.         |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Dropout ratio             | None          | No dropout is used following Jaegle et al. (2022).               |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Embedding dimension       | None          | No Embedding layer is used, as illustrated in Section 4.1.       |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Optimizer                 | LAMB          | An optimizer specifically designed for Transformer-based models. |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Learning rate             | 0.001         | Parameter of the LAMB optimizer.                                 |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Weight decay rate         | 0.01          | Parameter of the LAMB optimizer.                                 |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Training steps            | 10\ :sup:`4`\ | Training times.                                                  |
    +---------------------------+---------------+------------------------------------------------------------------+
    | Episode length (T)        | 50            | The length of an episode.                                        |
    +---------------------------+---------------+------------------------------------------------------------------+
    | G                         | m/2           | Half of the assets are identified as winners.                    |
    +---------------------------+---------------+------------------------------------------------------------------+
    | W                         | 100           | The look-back window size.                                       |
    +---------------------------+---------------+------------------------------------------------------------------+


LSTM
~~~~

The LSTM model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced LSTM-based models.

RNN
~~~

The RNN model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced RNN-based models.

TCN
~~~

The TCN model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset. The model applies a series of fully connected layers to the input,
with each layer followed by a ReLU activation and a dropout layer.

The TCN model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset. The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced TCN-based models.


Transformer
~~~~~~~~~~~

The Transformer model takes an input tensor ``x`` of shape ``(batch_size, num_assets, num_features_augmented)``,
where ``num_features_augmented`` represents the number of features (including any preprocessed or augmented
features) for each asset.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced Transformer-based models.

.. _image_representation:

Image Representation Models
-------------------------------
In addition to time series data, ``FinOL`` also provides image representation models.
These models are designed to extract meaningful representations from financial images,
such as charts, graphs, and other visual data.  By doing so, it enables researchers to harness the wealth of
information contained in visual formats, which can be crucial for tasks like pattern recognition,
trend analysis, etc. The inclusion of this model in ``FinOL`` further solidifies its position as a versatile tool for
data-driven OLPS research.

CNN
~~~~

The CNN model takes an input tensor ``x`` of shape ``(batch_size, num_assets, height, width)``, where
``height`` and ``width`` are the dimensions of the image for each asset. The model applies a series of convolutional
layers to each asset's image, with each layer followed by a ReLU activation and a pooling layer to reduce
the spatial dimensions.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

.. note::

    Users can refer to this implementation and use it as a starting point for developing their own advanced CNN-based models.

CNN-JF
~~~~~~

.. figure:: ../../images/models/CNN-JF.png
   :align: center
   :width: 550px

   Overall Framework of CNN-JF

The CNN-JF model is a CNN-based model for asset scoring and portfolio selection. It leverages CNN to analyze
historical stock price data represented as images.

The CNN-JF model takes an input tensor ``x`` of shape ``(batch_size, num_assets, height, width)``, where
``height`` and ``width`` are the dimensions of the image for each asset. The model applies a series of convolutional
layers to each asset's image, with each layer followed by a leaky ReLU activation and a pooling layer to reduce
the spatial dimensions.

The final output of the model is a tensor of shape ``(batch_size, num_assets)``, where each element
represents the predicted score for the corresponding asset.

For more details, please refer to the paper `(Re-)Imag(in)ing Price Trends <https://onlinelibrary.wiley.com/doi/epdf/10.1111/jofi.13268>`__.

.. table:: Hyper-parameters of (Re-)Imag(in)ing Price Trends.
    :class: ghost

    +----------------------+--------+-------------------+--------+
    | Hyper-parameter      | Choice | Hyper-parameter   | Choice |
    +======================+========+===================+========+
    | Kernel Size Height   | 5      | Kernel Size Width | 3      |
    +----------------------+--------+-------------------+--------+
    | Stride Height        | 3      | Stride Width      | 1      |
    +----------------------+--------+-------------------+--------+
    | Dilation Height      | 2      | Dilation Width    | 1      |
    +----------------------+--------+-------------------+--------+
    | Padding Height       | 12     | Padding Width     | 1      |
    +----------------------+--------+-------------------+--------+
    | Dropout Rate         | 0.5    |                   |        |
    +----------------------+--------+-------------------+--------+


